{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kavas\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data  # Features\n",
    "y = data.target  # Labels\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def softmax(Z):\n",
    "    exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "    return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy loss function\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    n_samples = y_true.shape[0]\n",
    "    logp = - np.log(y_pred[range(n_samples), np.argmax(y_true, axis=1)])\n",
    "    loss = np.sum(logp) / n_samples\n",
    "    return loss\n",
    "\n",
    "# Accuracy\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        # Initialize weights\n",
    "        self.W1 = np.random.randn(input_size, hidden_size1) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size1))\n",
    "        self.W2 = np.random.randn(hidden_size1, hidden_size2) * 0.01\n",
    "        self.b2 = np.zeros((1, hidden_size2))\n",
    "        self.W3 = np.random.randn(hidden_size2, output_size) * 0.01\n",
    "        self.b3 = np.zeros((1, output_size))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Forward propagation\n",
    "        self.Z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.A1 = relu(self.Z1)\n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = relu(self.Z2)\n",
    "        self.Z3 = np.dot(self.A2, self.W3) + self.b3\n",
    "        self.A3 = softmax(self.Z3)\n",
    "        return self.A3\n",
    "    \n",
    "    def backward(self, X, y, y_pred):\n",
    "        # Backward propagation (manually computed gradients)\n",
    "        m = y.shape[0]\n",
    "\n",
    "        # Output layer gradient\n",
    "        dZ3 = y_pred - y\n",
    "        self.dW3 = np.dot(self.A2.T, dZ3) / m\n",
    "        self.db3 = np.sum(dZ3, axis=0, keepdims=True) / m\n",
    "\n",
    "        # Second hidden layer gradient\n",
    "        dA2 = np.dot(dZ3, self.W3.T)\n",
    "        dZ2 = dA2 * relu_derivative(self.Z2)\n",
    "        self.dW2 = np.dot(self.A1.T, dZ2) / m\n",
    "        self.db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "\n",
    "        # First hidden layer gradient\n",
    "        dA1 = np.dot(dZ2, self.W2.T)\n",
    "        dZ1 = dA1 * relu_derivative(self.Z1)\n",
    "        self.dW1 = np.dot(X.T, dZ1) / m\n",
    "        self.db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, nn, learning_rate=0.01, optimizer='sgd'):\n",
    "        self.nn = nn\n",
    "        self.lr = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Adam optimizer variables\n",
    "        if optimizer == 'adam':\n",
    "            self.mW1 = np.zeros_like(nn.W1)\n",
    "            self.vW1 = np.zeros_like(nn.W1)\n",
    "            self.mW2 = np.zeros_like(nn.W2)\n",
    "            self.vW2 = np.zeros_like(nn.W2)\n",
    "            self.mW3 = np.zeros_like(nn.W3)\n",
    "            self.vW3 = np.zeros_like(nn.W3)\n",
    "            self.mb1 = np.zeros_like(nn.b1)\n",
    "            self.vb1 = np.zeros_like(nn.b1)\n",
    "            self.mb2 = np.zeros_like(nn.b2)\n",
    "            self.vb2 = np.zeros_like(nn.b2)\n",
    "            self.mb3 = np.zeros_like(nn.b3)\n",
    "            self.vb3 = np.zeros_like(nn.b3)\n",
    "            self.beta1 = 0.9\n",
    "            self.beta2 = 0.999\n",
    "            self.eps = 1e-8\n",
    "            self.t = 0\n",
    "    \n",
    "    def update(self):\n",
    "        if self.optimizer == 'sgd':\n",
    "            # SGD updates\n",
    "            self.nn.W1 -= self.lr * self.nn.dW1\n",
    "            self.nn.b1 -= self.lr * self.nn.db1\n",
    "            self.nn.W2 -= self.lr * self.nn.dW2\n",
    "            self.nn.b2 -= self.lr * self.nn.db2\n",
    "            self.nn.W3 -= self.lr * self.nn.dW3\n",
    "            self.nn.b3 -= self.lr * self.nn.db3\n",
    "        \n",
    "        elif self.optimizer == 'adam':\n",
    "            # Adam updates\n",
    "            self.t += 1\n",
    "            self._adam_update('W1', 'b1')\n",
    "            self._adam_update('W2', 'b2')\n",
    "            self._adam_update('W3', 'b3')\n",
    "    \n",
    "    def _adam_update(self, weight, bias):\n",
    "        W = getattr(self.nn, weight)\n",
    "        dW = getattr(self.nn, 'd' + weight)\n",
    "        mW = getattr(self, 'm' + weight)\n",
    "        vW = getattr(self, 'v' + weight)\n",
    "        \n",
    "        b = getattr(self.nn, bias)\n",
    "        db = getattr(self.nn, 'd' + bias)\n",
    "        mb = getattr(self, 'm' + bias)\n",
    "        vb = getattr(self, 'v' + bias)\n",
    "        \n",
    "        # Update biased moments\n",
    "        mW = self.beta1 * mW + (1 - self.beta1) * dW\n",
    "        vW = self.beta2 * vW + (1 - self.beta2) * (dW ** 2)\n",
    "        mb = self.beta1 * mb + (1 - self.beta1) * db\n",
    "        vb = self.beta2 * vb + (1 - self.beta2) * (db ** 2)\n",
    "        \n",
    "        # Bias correction\n",
    "        mW_hat = mW / (1 - self.beta1 ** self.t)\n",
    "        vW_hat = vW / (1 - self.beta2 ** self.t)\n",
    "        mb_hat = mb / (1 - self.beta1 ** self.t)\n",
    "        vb_hat = vb / (1 - self.beta2 ** self.t)\n",
    "        \n",
    "        # Update parameters\n",
    "        W -= self.lr * mW_hat / (np.sqrt(vW_hat) + self.eps)\n",
    "        b -= self.lr * mb_hat / (np.sqrt(vb_hat) + self.eps)\n",
    "        \n",
    "        # Save updated moments\n",
    "        setattr(self, 'm' + weight, mW)\n",
    "        setattr(self, 'v' + weight, vW)\n",
    "        setattr(self, 'm' + bias, mb)\n",
    "        setattr(self, 'v' + bias, vb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, X_test, y_test, optimizer='sgd', epochs=100, lr=0.01):\n",
    "    # Initialize the neural network\n",
    "    input_size = X_train.shape[1]\n",
    "    hidden_size1 = 64\n",
    "    hidden_size2 = 32\n",
    "    output_size = y_train.shape[1]\n",
    "    nn = NeuralNetwork(input_size, hidden_size1, hidden_size2, output_size)\n",
    "    \n",
    "    # Initialize the optimizer\n",
    "    opt = Optimizer(nn, learning_rate=lr, optimizer=optimizer)\n",
    "    \n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        y_pred = nn.forward(X_train)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = cross_entropy_loss(y_train, y_pred)\n",
    "        train_loss.append(loss)\n",
    "        \n",
    "        # Backward pass\n",
    "        nn.backward(X_train, y_train, y_pred)\n",
    "        \n",
    "        # Update weights\n",
    "        opt.update()\n",
    "        \n",
    "        # Test loss\n",
    "        y_test_pred = nn.forward(X_test)\n",
    "        loss_test = cross_entropy_loss(y_test, y_test_pred)\n",
    "        test_loss.append(loss_test)\n",
    "        if epoch % 10 == 0:\n",
    "            acc = accuracy(y_test, y_test_pred)\n",
    "            print(f'Epoch {epoch}, Loss: {loss:.4f}, Test Loss: {loss_test:.4f}, Test Accuracy: {acc:.4f}')\n",
    "\n",
    "    return train_loss, test_loss\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
